{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5JnVCL-M50z"
   },
   "source": [
    "<h2>Convolutional Neural Networks: Image classification of brain tumor MRI data</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_HvJxGhji0d"
   },
   "source": [
    "In this practical session we will use the following dataset: https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri/data. This is a dataset of magnetic resonance images of the brain of patients with the following (or absence thereof) conditions:\n",
    "- glioma\n",
    "- meningioma\n",
    "- pituitary tumor\n",
    "- healthy brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FysQskQblUSM"
   },
   "source": [
    "**Exercise** 1\n",
    "\n",
    "Write the code that you will use to load the training and test data and build the two datasets. The images are found in the brain_mri_data folder and they are already organized in train and test subfolders. Use python's `os` or `pathlib` libraries to iterate over the folders and access the data and use the `open` function from Python Imaging Library (PIL) to open the images.\n",
    "\n",
    "Each dataset must be a list of tuples, with each tuple having two values: an image in torch tensor format (type float32), and a label, which must be a 4-dimensional torch tensor of type float32 where [1,0,0,0] is the first class (glioma), [0,1,0,0] is the second class (meningioma), [0,0,1,0] is the third class (pitutary tumor) and [0,0,0,1] is the fourth class (healthy brain). Since the images are of different sizes, you need to use the `.resize()` method from PIL to resize them so that they are all of shape (128, 128).\n",
    "\n",
    "Now convert the images to numpy arrays. If you print the shape you will see that it is 128x128x3, the 3 corresponding to the 3 channels in an RGB image. In PyTorch we need the channels to be the first dimension. Before converting the image to the final torch tensor format, use np.rollaxis(img, -1) to roll the last axis to the front and obtain an image of 3x128x128, then convert it to a torch tensor of type float32.\n",
    "\n",
    "Finally, standardize each image by subtracting a mean of 63.64 and dividing by a standard deviation of 54.89 calculated from the images in the training set. Store the training and test sets in variables called `train_set` and `test_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bKIcFvGhn4m-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "tumor_types = ['glioma_tumor', 'no_tumor', 'pituitary_tumor', 'meningioma_tumor']\n",
    "splits = ['Testing', 'Training']\n",
    "# dataset = ([], []) / uncomment this line if you want to use tuple unpacking to build your datasets\n",
    "train_set, test_set = [], []\n",
    "\n",
    "for tumor in tumor_types:\n",
    "    for split in splits:\n",
    "        dir_path = f'brain_mri/{split}/{tumor}'\n",
    "        for img in os.listdir(dir_path):\n",
    "            if img.endswith('.jpg'):\n",
    "                img_pil = Image.open(f'{dir_path}/{img}').resize((128, 128))\n",
    "                image = (torch.tensor(np.rollaxis(np.array(img_pil), -1), dtype=torch.float32) - 63.64) / 54.89\n",
    "                label = torch.zeros(4, dtype=torch.float32)\n",
    "                label[tumor_types.index(tumor)] = 1\n",
    "                if split == 'Testing':\n",
    "                    test_set.append((image, label))\n",
    "                elif split == 'Training':\n",
    "                    train_set.append((image, label))\n",
    "      # write your code here\n",
    "      # loading the images might somewhere between 30 seconds and 3 minutes\n",
    "\n",
    "# train_set, test_set = dataset / uncomment this line if you want to use tuple unpacking to build your datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7FJ6JEjoH7R"
   },
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Build your model. It must have the following structure:\n",
    "\n",
    "- A first convolutional layer with 3 input filters, 16 output filters, kernel size of 5 and padding of 2\n",
    "- A second convolutional layer with 16 input filters, 16 output filters, kernel size of 3 and padding of 1\n",
    "- A batch normalization operation with 16 input features\n",
    "- A max pooling operation with kernel size 2 and stride 2\n",
    "- A third convolutional layer with 16 input filters, 32 output filters, kernel size of 3 and padding of 1\n",
    "- A fourth convolutional layer with 32 input filters, 32 output filters, kernel size of 3 and padding of 1\n",
    "- A batch normalization operation with 32 input features\n",
    "- A second max pooling operation with kernel size 2 and stride 2\n",
    "- A fifth convolutional layer with 32 input filters, 64 output filters, kernel size of 3 and padding of 1\n",
    "- A batch normalization operation with 64 input features\n",
    "- An adaptive average pooling operation with output size of 4\n",
    "- A linear layer with 1024 input neurons and 16 output neurons\n",
    "- A second linear layer with 16 input neurons and 4 output neurons\n",
    "- A softmax activation function at the last layer (`F.softmax(x, dim=1)`)\n",
    "\n",
    "Use ReLU as the activation function (`F.relu(x)`) after each convolutional and linear layer except the last one, where you must use SoftMax. You can read how to use batch normalization in PyTorch's documentation: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "\n",
    "After applying the adaptive average pooling in the forward method, you must reshape the tensor to have shape (-1, 1024). Using -1 when you reshape tensor will infer the first dimension from the batch size. The output should be a tensor of shape `(batch size, 4)` that you can compare with the label, where 4 corresponds to the number of classes.\n",
    "\n",
    "Pytorch has a tutorial on how to train a classifier with convolutional layers:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "If at any moment you are in doubt about how the information flows, print the dimensions of the vector `x` at key steps in the forward method with `print(x.shape)`. This will help you understand the dimensions of your data as you apply the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "testloader = DataLoader(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2870"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 128, 128])\n",
      "torch.Size([100, 16, 128, 128])\n",
      "torch.Size([100, 32, 128, 128])\n",
      "torch.Size([100, 32, 64, 64])\n",
      "torch.Size([100, 64, 64, 64])\n",
      "torch.Size([100, 64, 64, 64])\n",
      "torch.Size([100, 64, 32, 32])\n",
      "torch.Size([100, 64, 4, 4])\n",
      "torch.Size([100, 1024])\n",
      "torch.Size([100, 32])\n",
      "torch.Size([100, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(100, 3, 128, 128)\n",
    "conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
    "conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "avg_pool = nn.AdaptiveAvgPool2d(4)\n",
    "linear_1 = nn.Linear(1024, 32)\n",
    "linear_2 = nn.Linear(32, 4)\n",
    "\n",
    "\n",
    "print(x.shape)\n",
    "x = F.relu(conv1(x))\n",
    "print(x.shape)\n",
    "x = F.relu(conv2(x))\n",
    "print(x.shape)\n",
    "x = maxpool(x)\n",
    "print(x.shape)\n",
    "x = F.relu(conv3(x))\n",
    "print(x.shape)\n",
    "x = F.relu(conv4(x))\n",
    "print(x.shape)\n",
    "x = maxpool(x)\n",
    "print(x.shape)\n",
    "x = avg_pool(x)\n",
    "print(x.shape)\n",
    "x = x.reshape(100, 1024)\n",
    "print(x.shape)\n",
    "x = linear_1(x)\n",
    "print(x.shape)\n",
    "x = linear_2(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 * 4 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(64, 3, 128, 128)\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "linear1 = nn.Linear(256, 64)\n",
    "linear_2 = nn.Linear(64, 4)\n",
    "avgpool = nn.AdaptiveAvgPool2d(2)\n",
    "batch_norm_1 = nn.BatchNorm2d(16)\n",
    "batch_norm_2 = nn.BatchNorm2d(32)\n",
    "batch_norm_3 = nn.BatchNorm2d(64)\n",
    "x = F.relu(conv1(x))\n",
    "x = batch_norm_1(x)\n",
    "x = F.relu(conv2(x))\n",
    "x = batch_norm_2(x)\n",
    "x = maxpool(x)\n",
    "x = F.relu(conv3(x))\n",
    "x = batch_norm_3(x)\n",
    "x = avgpool(x)\n",
    "x = x.reshape(-1, 256)\n",
    "x = F.relu(linear1(x))\n",
    "x = linear_2(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_array.reshape(4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "l19ZdRDUrdow"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvolutionalNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(4)\n",
    "        self.linear_1 = nn.Linear(1024, 32)\n",
    "        self.linear_2 = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(conv1(x))\n",
    "        x = F.relu(conv2(x))\n",
    "        x = maxpool(x)\n",
    "\n",
    "        x = F.relu(conv3(x))\n",
    "        x = F.relu(conv4(x))\n",
    "        x = maxpool(x)\n",
    "\n",
    "        x = avg_pool(x)\n",
    "        x = x.reshape(-1, 1024)\n",
    "\n",
    "        x = F.relu(linear_1(x))\n",
    "        x = F.softmax(linear_2(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_number_of_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fOESG4d0yDl"
   },
   "source": [
    "**Exercise 3**\n",
    "\n",
    "Initialize your model by creating an instance of the ConvolutionalNN class and print the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZ4vMe0uaWif",
    "outputId": "73843ed2-7a5e-4ac8-8448-2d7426182ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model number of parameters:\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = ConvolutionalNN().to(device)\n",
    "\n",
    "print('model number of parameters:') # write the code to print the number of parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1,3,128,128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7e2kKTFrvYv"
   },
   "source": [
    "**Exercise 4**\n",
    "\n",
    "Define the DataLoader, optimizer and loss function. Use a batch size of 32 for the training loader with `shuffle=True` and a batch size of 1 for the test loader with `shuffle=False`. Name the dataloader variables `train_loader` and `test_loader`. We will ignore PyTorch's Dataset class for now.\n",
    "\n",
    "Use cross entropy as the loss function and Adam as optimizer with a learning rate of 5e-4 and weight decay of 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "VRtdZUa1tVKb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# define the train loader and test loader\u001b[39;00m\n\u001b[1;32m      7\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m----> 9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "# define the train loader and test loader\n",
    "\n",
    "loss_function = CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIw7SY8e1STZ"
   },
   "source": [
    "**Exercise 5**\n",
    "\n",
    "Train the model for 60 epochs. What is the best accuracy that you obtain in the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2gTBQej1H2e"
   },
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "n_samples_train = len(train_loader)\n",
    "n_samples_test = len(test_loader)\n",
    "training_loss_per_epoch, test_loss_per_epoch, test_accuracy_per_epoch = [], [], []\n",
    "best_accuracy = - torch.inf\n",
    "for epoch in range(epochs):\n",
    "    training_loss = 0\n",
    "    for data, labels in train_loader:\n",
    "        predict = model(data.to(device))\n",
    "        loss = loss_function(labels.to(device), predict)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += (loss.item() / n_samples_train)\n",
    "    test_loss, correct, total = (0, 0, 0)\n",
    "    for data, label in test_loader:\n",
    "        predict = model(data.to(device))\n",
    "        loss = loss_function(label.to(device), predict)\n",
    "        test_loss += (loss.item() / n_samples_test)\n",
    "        if torch.argmax(predict) == torch.argmax(label):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct/total*100\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "    training_loss_per_epoch.append(training_loss)\n",
    "    test_loss_per_epoch.append(test_loss)\n",
    "    test_accuracy_per_epoch.append(accuracy)\n",
    "    print('''Epoch {}: training loss: {:.3f}, test loss: {:.3f}, test accuracy: {:.2f}%\n",
    "          '''.format(epoch+1, training_loss, test_loss, accuracy))\n",
    "print(f'\\n Best test accuracy: {best_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLN_XPIM3AET"
   },
   "source": [
    "**Exercise 6**\n",
    "\n",
    "Print the training loss per epoch and the test loss per epoch. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Smd9RIAP2_u6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(test_loss_per_epoch, c='r')\n",
    "ax[0].set_xlabel('Epochs', fontsize=16)\n",
    "ax[0].set_ylabel('Loss', fontsize=16)\n",
    "ax[1].plot(test_accuracy_per_epoch, c='g')\n",
    "ax[1].set_xlabel('Epochs', fontsize=16)\n",
    "ax[1].set_ylabel('Accuracy %', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlE_K4c13JDh"
   },
   "source": [
    "**Exercise 7**\n",
    "\n",
    "Go back to the neural network that you designed in exercise 2. Using a model with a maximum of 50.000 parameters, train a neural network to obtain the maximum possible accuracy in the test set. Use a maximum of 150 epochs with a learning rate of 5e-4 and a batch size of 32. You can also tune the `weight_decay` hyperparameter in the Adam optimizer to have the value of your choice. Additionally, you can try to resize the images to have different height and width and see if this improves the performance of the model. Who will achieve the maximum accuracy? I have witnessed test accuracies of up to ~70% with this setup.\n",
    "\n",
    "You can use Dropout, Batch Normalization and Maxpooling operations which don't add any parameters to your model and might improve generalization and therefore the accuracy in the test set.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
